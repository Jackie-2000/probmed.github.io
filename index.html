<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="Worse than Random? An Embarrassingly Simple Probing Evaluation of Large Multimodal Models in Medical VQA">
  <meta name="keywords" content="ProbMed, Prob Med">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title> Worse than Random? An Embarrassingly Simple Probing Evaluation of Large Multimodal Models in Medical VQA</title>

  <!-- Global site tag (gtag.js) - Google Analytics -->
  <!-- <script async src="https://www.googletagmanager.com/gtag/js?id=G-PYVRSFMDRL"></script> -->
  <!-- <script>
    window.dataLayer = window.dataLayer || [];

    function gtag() {
      dataLayer.push(arguments);
    }
/Users/panlu/Library/Mobile Documents/com~apple~CloudDocs/ImageMath/visual-mathqa-server/data_final/images
    gtag('js', new Date());

    gtag('config', 'G-PYVRSFMDRL');
  </script> -->

  <link rel="icon" href="./static/images/mathvista.png">

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro" rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="stylesheet" href="./static/css/leaderboard.css">

  <!-- <link href="https://unpkg.com/tabulator-tables@5.5.2/dist/css/tabulator_bulma.min.css" rel="stylesheet">
  <script type="text/javascript" src="https://unpkg.com/tabulator-tables@5.5.2/dist/js/tabulator.min.js"></script> -->
  <script type="text/javascript" src="static/js/sort-table.js" defer></script>

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/explorer-index.js"></script>
  <script src="./static/js/question_card.js"></script>

  <script src="./static/js/leaderboard_testmini.js"></script>  
  <script src="./data/results/output_folders.js" defer></script>
  <script src="./data/results/model_scores.js" defer></script>

  <script src="./visualizer/data/data_public.js" defer></script>
</head>
<body>

<!-- Heading -->
<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title">Worse than Random? An Embarrassingly Simple Probing Evaluation of Large Multimodal Models in Medical VQA</h1>
          <div class="is-size-5 publication-authors">
            <span class="author-block">
              <a href="https://Jackie-2000.github.io/">Qianqi Yan</a><sup style="color:#6fbf73;">1</sup>,</span>
            <span class="author-block">
              <a href="https://scholar.google.com/citations?user=kDzxOzUAAAAJ&hl=en">Xuehai He</a><sup style="color:#6fbf73;">1</sup>,</span>
            <span class="author-block">
              <a href="https://xiangyue9607.github.io/">Xiang Yue</a><sup style="color:#ed4b82">2</sup>,
            </span>
            <span class="author-block">
              <a href="https://eric-xw.github.io/">Xin Eric Wang</a><sup style="color:#6fbf73;">1</sup>,
            </span>
          </div>

          <div class="is-size-5 publication-authors">
            <span class="author-block"><sup style="color:#6fbf73;">1</sup>University of California, Santa Cruz,</span><br>
            <span class="author-block"><sup style="color:#ed4b82">2</sup>Carnegie Mellon University</span>
          </div>

          <div class="column has-text-centered">
            <div class="publication-links">
              <!-- PDF Link. -->
              <span class="link-block">
                <a href="https://github.com/eric-ai-lab/ProbMed"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                  </span>
                  <span>Paper</span>
                </a>
              </span>
              <span class="link-block">
                <a href="https://github.com/eric-ai-lab/ProbMed"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span>
              <!-- Code Link. -->
              <span class="link-block">
                <a href="https://github.com/eric-ai-lab/ProbMed"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-github"></i>
                  </span>
                  <span>Code</span>
                  </a>
              </span>
              <!-- Dataset Link. -->
              <span class="link-block">
                <a href="https://github.com/eric-ai-lab/ProbMed"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <!-- <i class="far fa-images"></i> -->
                      <p style="font-size:18px">🤗</p>
                      <!-- 🔗 -->
                  </span>
                  <span>Dataset</span>
                </a>
              </span>
            </div>

          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<!-- Teaser figures -->
<section class="section">
  <div class="container" style="margin-top: -150px; margin-bottom: -100px;">
    <div class="columns is-centered m-6">
      <div class="column is-full has-text-centered content">
        <div id="results-carousel" class="carousel results-carousel">
          <div class="box m-5">
            <div class="content has-text-centered">
              <img src="static/images/teaser_example.png" alt="adversarial example" width="84%"/>
              <p> An example illustrating the potential for misleading accuracy in existing evaluations.  While the model correctly identifies the position of an existing finding in the standard evaluation, it fails to differentiate between actual and hallucinated positions when subjected to an adversarial evaluation.
              </p>
            </div>
          </div>
          <div class="box m-5">
            <div class="content has-text-centered">
              <img src="static/images/teaser_stats.png" alt="stats adversarial" width="84%"/>
              <p> Accuracy of four LMMs on two types of specialized questions in medical diagnoses, with and without adversarial pairs. The significant drop in accuracy with adversarial pairs highlights the models' unreliability in handling medical diagnoses.
              </p>
            </div>
          </div>
        </div>
      </div>
    </div>
</section>

<!-- Abstract -->
<section class="section">
  <div class="container" style="margin-bottom: 2vh;">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Introduction</h2>
        <div class="content has-text-justified">
          <p>
            Large Multimodal Models (LMMs) have shown remarkable progress in the field of medical Visual Question Answering (Med-VQA), achieving high accuracy on existing benchmarks. However, their reliability under robust evaluation is questionable. This study reveals that state-of-the-art models, when subjected to simple probing evaluation, perform worse than random guessing on medical diagnosis questions. 
          </p>
          <p>
            To address this critical evaluation problem, we introduce the <b>Probing Evaluation for Medical Diagnosis (ProbMed)</b> dataset to rigorously assess LMM performance in medical imaging through <b>probing evaluation</b> and <b>procedural diagnosis</b>. Particularly, probing evaluation features pairing original questions with negation questions with hallucinated attributes, while procedural diagnosis requires reasoning across various diagnostic dimensions for each image, including modality recognition, organ identification, clinical findings, abnormalities, and positional grounding. 
          </p>
          <p>
            Our evaluation reveals that top-performing models like GPT-4V and Gemini Pro perform worse than random guessing on specialized diagnostic questions, indicating significant limitations in handling fine-grained medical inquiries. Besides, models like LLaVA-Med struggle even with more general questions, and results from CheXagent demonstrate the transferability of expertise across different modalities of the same organ, showing that specialized domain knowledge is still crucial for improving performance. This study underscores the urgent need for more robust evaluation to ensure the reliability of LMMs in critical fields like medical diagnosis, and current LMMs are still far from applicable to those fields.
          </p>
        </div>
      </div>
    </div>
    <!--/ Abstract. -->
</div>
</section>

<!-- <section class="section">
  <div class="container">
    
    <div class="columns is-centered">
      <div class="column is-full has-text-centered content">
        <h2 class="title is-3" id="leaderboard_test">Leaderboard on ProbMed</h2>
        <div class="content">
          <p class="mt-3">Accuracy scores on the <b>test</b> subset (5,141 examples with <b>private</b> ground truth) of <img src="static/images/mathvista.png" style="width:1.0em;vertical-align: middle" alt="Logo"/>
            <span class="mathvista">MathVista</span>.
          </p>

          <table class="js-sort-table" id="results">
            <tr>
                <td class="js-sort-number"><strong>#</strong></td>
                <td class="js-sort-number"><strong>Model</strong></td>
                <td class="js-sort-number"><strong>Method</strong></td>
                <td class="js-sort-number"><strong>Source</strong></td>
                <td class="js-sort-number"><strong>Date</strong></td>
                <td class="js-sort-number"><strong><u>ALL</u></strong></td>
                <td class="js-sort-number"><strong>FQA</strong></td>
                <td class="js-sort-number"><strong>GPS</strong></td>
                <td class="js-sort-number"><strong>MWP</strong></td>
                <td class="js-sort-number"><strong>TQA</strong></td>
                <td class="js-sort-number"><strong>VQA</strong></td>
                <td class="js-sort-number"><strong>ALG</strong></td>
                <td class="js-sort-number"><strong>ARI</strong></td>
                <td class="js-sort-number"><strong>GEO</strong></td>
                <td class="js-sort-number"><strong>LOG</strong></td>
                <td class="js-sort-number"><strong>NUM</strong></td>
                <td class="js-sort-number"><strong>SCI</strong></td>
                <td class="js-sort-number"><strong>STA</strong></td>
            </tr>
            <tr>
              <td>1</td>
              <td><b class="best-score-text">InternVL-Chat-V1.2-Plus 🥇</b></td>
              <td>LMM 🖼️</td>
              <td><a href="https://arxiv.org/abs/2312.14238" class="ext-link" style="font-size: 16px;">Link</a></td>
              <td>2024-02-22</td>
              <td><b class="best-score-text">60.18</b></td>
              <td>52.2</td>
              <td>56.2</td>
              <td>78.3</td>
              <td>61.6</td>
              <td>55.5</td>
              <td>56.0</td>
              <td>64.4</td>
              <td>57.6</td>
              <td>21.6</td>
              <td>46.1</td>
              <td>60.0</td>
              <td>60.1</td>                                
            </tr>
            <tr>
              <td>2</td>
              <td><b class="best-score-text">InternLM-XComposer2-VL-7B 🥈</b></td>
              <td>LMM 🖼️</td>
              <td><a href="https://github.com/InternLM/InternLM-XComposer" class="ext-link" style="font-size: 16px;">Link</a></td>
              <td>2024-01-22</td>
              <td><b class="best-score-text">57.93</b></td>
              <td>53.9</td>
              <td>56.4</td>
              <td>77.1</td>
              <td>58.4</td>
              <td>43.2</td>
              <td>54.8</td>
              <td>57.6</td>
              <td>58.0</td>
              <td>16.5</td>
              <td>47.6</td>
              <td>59.1</td>
              <td>62.5</td>                                  
            </tr>
            <tr>
              <td>3</td>
              <td><b class="best-score-text">Qwen-VL-Plus 🥉</b></td>
              <td>LMM 🖼️</td>
              <td><a href="https://github.com/QwenLM/Qwen-VL" class="ext-link" style="font-size: 16px;">Link</a></td>
              <td>2023-12-26</td>
              <td><b class="best-score-text">44.33</b></td>
              <td>55.9</td>
              <td>34.7</td>
              <td>29.7</td>
              <td>58.8</td>
              <td>42.4</td>
              <td>40.7</td>
              <td>35.4</td>
              <td>36.6</td>
              <td>21.6</td>
              <td>30.4</td>
              <td>55.9</td>
              <td>56.3</td>                                  
            </tr>
            <tr>
              <td>4</td>
              <td><b class="">SPHINX-MoE</b></td>
              <td>MoE 🤖</td>
              <td><a href="https://github.com/Alpha-VLLM/LLaMA2-Accessory/tree/main/SPHINX" class="ext-link" style="font-size: 16px;">Link</a></td>
              <td>2024-01-13</td>
              <td><b class="">42.68</b></td>
              <td>50.3</td>
              <td>29.7</td>
              <td>40.9</td>
              <td>49.3</td>
              <td>43.3</td>
              <td>33.9</td>
              <td>43.0</td>
              <td>29.1</td>
              <td>14.4</td>
              <td>26.3</td>
              <td>46.9</td>     
              <td>51.2</td>                            
            </tr>
            <tr>
              <td>5</td>
              <td><b class="">MiniCPM-V-2 (2.8B)</b></td>
              <td>LMM 🖼️</td>
              <td><a href="https://github.com/OpenBMB/MiniCPM-V" class="ext-link" style="font-size: 16px;">Link</a></td>
              <td>2024-04-14</td>
              <td><b class="">39.89</b></td>
              <td>51.7</td>
              <td>27.4</td>
              <td>39.8</td>
              <td>42.5</td>
              <td>34.7</td>
              <td>31.3</td>
              <td>34.4</td>
              <td>30.7</td>
              <td>13.4</td>
              <td>33.5</td>
              <td>38.5</td>
              <td>50.0</td>                                  
            </tr>
            <tr>
              <td>6</td>
              <td><b class="">PoT GPT-4 (Caption+OCR)</b></td>
              <td>Tool 🛠️</td>
              <td><a href="https://arxiv.org/abs/2310.02255" class="ext-link" style="font-size: 16px;">Link</a></td>
              <td>2023-10-03</td>
              <td><b class="">31.74</b></td>
              <td>27.6</td>
              <td>37.4</td>
              <td>23.9</td>
              <td>43.0</td>
              <td>30.3</td>
              <td>37.1</td>
              <td>27.9</td>
              <td>37.5</td>
              <td>22.7</td>
              <td>15.8</td>
              <td>44.5</td>
              <td>31.9</td>                                  
            </tr>
            <tr>
              <td>7</td>
              <td><b>CoT GPT4 (Caption+OCR)</b></td>
              <td>Tool 🛠️</td>
              <td><a href="https://arxiv.org/abs/2310.02255" class="ext-link" style="font-size: 16px;">Link</a></td>
              <td>2023-10-03</td>
              <td><b>30.50</b></td>
              <td>27.2</td>
              <td>35.9</td>
              <td>21.3</td>
              <td>43.1</td>
              <td>28.2</td>
              <td>35.7</td>
              <td>25.2</td>
              <td>35.8</td>
              <td>24.7</td>
              <td>15.4</td>
              <td>47.3</td>
              <td>31.3</td>                               
            </tr>
            <tr>
              <td>8</td>
              <td><b>LLaVA (LLaMA-2-13B)</b></td>
              <td>LMM 🖼️</td>
              <td><a href="https://arxiv.org/abs/2310.02255" class="ext-link" style="font-size: 16px;">Link</a></td>
              <td>2023-10-03</td>
              <td><b>25.40</b></td>
              <td>22.9</td>
              <td>24.6</td>
              <td>18.1</td>
              <td>35.8</td>
              <td>29.7</td>
              <td>26.9</td>
              <td>22.5</td>
              <td>24.4</td>
              <td>19.1</td>
              <td>19.1</td>
              <td>34.7</td>
              <td>21.6</td>                                 
            </tr>
            <tr>
              <td>*</td>
              <td><b>Random Chance</b></td>
              <td>-</td>
              <td><a href="https://arxiv.org/abs/2310.02255" class="ext-link" style="font-size: 16px;">Link</a></td>
              <td>2023-10-03</td>
              <td><b>17.86</b></td>
              <td>15.5</td>
              <td>24.1</td>
              <td>4.5</td>
              <td>23.4</td>
              <td>24.3</td>
              <td>25.8</td>
              <td>13.8</td>
              <td>22.7</td>
              <td>13.4</td>
              <td>8.8</td>
              <td>15.8</td>
              <td>14.3</td>                                                  
            </tr>                                                      
        </table>

          <b>Human Performance*:</b> Average human performance from AMT annotators who have high school diplomas or above.
          <br>
          <b>Method types:</b> <b>MoE 🤖:</b> Mixture of Experts, <b>LMM 🖼️:</b> Large Multimodal Model, <b>Tool 🛠️:</b> Tool-augmented Large Language Model.
          <br>
          <b>Task types:</b> <b>FQA:</b> figure QA,
          <b>GPS:</b> geometry problem solving,
          <b>MWP:</b> math word problem,
          <b>TQA:</b> textbook QA,
          <b>VQA:</b> visual QA.
          <br>
          <b>Math reasoning types:</b> 
          <b>ALG:</b> algebraic,
          <b>ARI:</b> arithmetic,
          <b>GEO:</b> geometry,
          <b>LOG:</b> logical ,
          <b>NUM:</b> numeric,
          <b>SCI:</b> scientific,
          <b>STA:</b> statistical.
          <br>
          <br>
          <div>
          <p>🚨 To submit your results to the leaderboard, please send to <a href="mailto:lupantech@gmail.com">this email</a> with your result json files.</p>
          <p>🚨 For more submission details, please refer to <a href="https://github.com/lupantech/MathVista?tab=readme-ov-file#-leaderboard-">this link</a> and <a href="https://github.com/lupantech/MathVista?tab=readme-ov-file#-evaluations-on-mathvista">this link</a>.
          </p>
          </div>
        </div>

      </div>
    </div>

  </div>
</section> -->

<!-- DATASET SECTION -->

<section class="hero is-light is-small">
  <div class="hero-body has-text-centered">
    <h1 class="title is-1 mathvista">ProbMed Benchmark</h1>
  </div>
</section>        
<section class="section">
  <div class="container">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Overview</h2>
        <div class="content has-text-justified">
          <p>
            ProbMed draws from two comprehensive biomedical datasets MedICaT and ChestX-ray14 to compile a diverse set of <b>6,303 images</b>. These images span three modalities (X-ray, MRI, and CT scan) and four organs (abdomen, brain, chest, and spine). After preprocessing, we generated a diverse set of high-quality questions for each image, covering various diagnostic dimensions. This process resulted in a total of <b>57,132 question-answer pairs</b>, averaging 9 pairs per image.
          </p>
          <ul>
            <li><b>Is the current evaluation of LMMs for Med-VQA reliable?</b> 
            <p>
              One of the main motivations behind ProbMed is to assess the models' ability to accurately distinguish between relevant and irrelevant features. ProbMed pairs original questions with negation questions containing hallucinated attributes. This method challenges the model robustness by requiring them to identify true conditions while disregarding false, hallucinated ones. For instance, a question about a specific finding is paired with a negated question featuring a different, non-existent finding to test if the model can exclusively identify the true finding.
            </p></li>
            <li><b>How reliable are LMMs on medical diagnosis, ranging from general questions to specialized diagnostic questions?</b>
            <p>
              To ensure a comprehensive evaluation, ProbMed includes questions that require reasoning across multiple diagnostic dimensions for each image. These dimensions include modality recognition, organ identification, clinical findings, abnormalities, and positional reasoning. This multifaceted approach assesses a model's diagnostic capabilities beyond simple question-answer pairs, requiring it to integrate various pieces of information to form a coherent diagnostic picture.
            </p></li>
          </ul>

          <div id="results-carousel" class="carousel results-carousel">
            <div class="box m-5">
              <div class="content has-text-centered">
                <img src="static/images/flow_diagram.png" alt="algebraic reasoning" width="80%"/>
                <p> Flow diagram of the ProbMed data curation process. Two comprehensive biomedical datasets were utilized to collect source data and construct a metadata file, enabling the automatic generation of high-quality question-answer pairs for the ProbMed dataset. </p>
              </div>
            </div>
            <div class="box m-5">
              <div class="content has-text-centered">
                <img src="static/images/table_stats.png" alt="arithmetic reasoning" width="80%"/>
                <p> Dataset Statistics of ProbMed. There are 6.3k images and 57k VQA pairs in total. The dataset is balanced within each question type and image type. </p>
              </div>
            </div>
          </div>

          <p>
            You can download the dataset on <a href="https://github.com/eric-ai-lab/ProbMed" target="_blank">Hugging Face Dataset</a>.
          </p>
        </div>
      </div>
    </div>
  </div>
</section>

<!-- RESULTS SECTION -->
<section class="hero is-light is-small">
  <div class="hero-body has-text-centered">
    <h1 class="title is-1 mathvista">Experimental Analysis</h1>
  </div>
</section>

<section class="section">
  <div class="container">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Is Current Evaluation of LMMs for Med-VQA Reliable?</h2>
        <div class="content has-text-justified">
          <ul>
            <li><b>Probing Evaluation with Adversarial Pairs in VQA-RAD</b> 
            <p>
              We construct adversarial pairs for 118 test instances where the answer is "yes" out of 272 closed-ended question-answers pairs within the test set of an existing benchmark. Each adversarial pair was manually created such that, based on the limited information from the original question-answer pair, the answer to the adversarial question had to be negated. This process resulted in 236 question-answer pairs in total. The adversarial questions in this subset are less challenging than those in ProbMed, as they often involve a simple semantic negation of the original question due to limited information.
            </p>
            <p>
              The results reveal the significant impact of adversarial pairs on model performance. Although the original accuracy appears very high for some underperforming models, the accuracy drops drastically after balancing the subset with adversarial pairs: 6.78% for GPT-4V and 16.95% for Gemini Pro, with an average decrease of <b>42.7%</b> across the tested models.
            </p></li>
            </li><b> Probing Evaluation with Adversarial Pairs in ProbMed</b>
            <p>
              Similar significant impact of adversarial pairs are observed in ProbMed. The accuracy of more capable models is generally less affected by the introduction of challenging adversarial pairs. However, even the most robust models experience <b>a minimum drop of 10.52%</b> in accuracy when tested with ProbMed's challenging questions, with an average decrease of <b>44.7%</b> across the tested models, highlighting the critical role of probing evaluation in evaluating Med-VQA performance comprehensively.
            </p></li>
          </ul>
        
        <div id="results-carousel" class="carousel results-carousel">
          <div class="box m-5">
            <div class="content has-text-centered">
              <img src="static/images/results_adv_RAD.png" alt="grade-lv" width="70%"/>
              <p>Model accuracy on the VQA-RAD test subset before and after introducing adversarial pairs. The table demonstrates the significant drop in accuracy across various models when adversarial pairs are added, highlighting the models' vulnerabilities to adversarial questions. The percentage decrease in accuracy is noted in parentheses.</p>
            </div>
          </div>
          <div class="box m-5">
            <div class="content has-text-centered">
              <img src="static/images/results_adv_probmed.png" alt="grade-lv" width="70%"/>
              <p>
                The impact of adversarial pairs on model accuracy in ProbMed. This table illustrates the performance change of models with the introduction of adversarial pairs. The results indicate a substantial decline in accuracy, underlining the robustness of ProbMed in challenging LMMs. The percentage drop in accuracy is noted in parentheses.</p>
            </div>
          </div>
        </div>
        
        </div>
      </div>
    </div>

    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3"> How Reliable Are LMMs in Medical Diagnosis?</h2>
        <div class="content has-text-justified">

        <div id="results-carousel" class="carousel results-carousel">
          <div class="box m-5">
            <div class="content has-text-centered">
              <img src="static/images/results-examples/5.png" alt="" width="60%"/>
            </div>
          </div>
          <div class="box m-5">
            <div class="content has-text-centered">
              <img src="static/images/results-examples/6.png" alt="" width="60%"/>
            </div>
          </div>
          <div class="box m-5">
            <div class="content has-text-centered">
              <img src="static/images/results-examples/37.png" alt="" width="60%"/>
            </div>
          </div>
          <div class="box m-5">
            <div class="content has-text-centered">
              <img src="static/images/results-examples/38.png" alt="" width="60%"/>
            </div>
          </div>
          <div class="box m-5">
            <div class="content has-text-centered">
              <img src="static/images/results-examples/39.png" alt="" width="60%"/>
            </div>
          </div>
          <div class="box m-5">
            <div class="content has-text-centered">
              <img src="static/images/results-examples/40.png" alt="" width="60%"/>
            </div>
          </div>
          <div class="box m-5">
            <div class="content has-text-centered">
              <img src="static/images/results-examples/41.png" alt="" width="60%"/>
            </div>
          </div>
          <div class="box m-5">
            <div class="content has-text-centered">
              <img src="static/images/results-examples/42.png" alt="" width="60%"/>
            </div>
          </div>
          <div class="box m-5">
            <div class="content has-text-centered">
              <img src="static/images/results-examples/43.png" alt="" width="60%"/>
            </div>
          </div>
          <div class="box m-5">
            <div class="content has-text-centered">
              <img src="static/images/results-examples/44.png" alt="" width="60%"/>
            </div>
          </div>
          <div class="box m-5">
            <div class="content has-text-centered">
              <img src="static/images/results-examples/45.png" alt="" width="60%"/>
            </div>
          </div>
          <div class="box m-5">
            <div class="content has-text-centered">
              <img src="static/images/results-examples/46.png" alt="" width="50%"/>
            </div>
          </div>
          <div class="box m-5">
            <div class="content has-text-centered">
              <img src="static/images/results-examples/47.png" alt="" width="60%"/>
            </div>
          </div>
          <div class="box m-5">
            <div class="content has-text-centered">
              <img src="static/images/results-examples/48.png" alt="" width="40%"/>
            </div>
          </div>
          <div class="box m-5">
            <div class="content has-text-centered">
              <img src="static/images/results-examples/49.png" alt="" width="40%"/>
            </div>
          </div>
          <div class="box m-5">
            <div class="content has-text-centered">
              <img src="static/images/results-examples/50.png" alt="" width="40%"/>
            </div>
          </div>
          <div class="box m-5">
            <div class="content has-text-centered">
              <img src="static/images/results-examples/51.png" alt="" width="40%"/>
            </div>
          </div>
          <div class="box m-5">
            <div class="content has-text-centered">
              <img src="static/images/results-examples/52.png" alt="" width="40%"/>
            </div>
          </div>
          <div class="box m-5">
            <div class="content has-text-centered">
              <img src="static/images/results-examples/53.png" alt="" width="40%"/>
            </div>
          </div>
          <div class="box m-5">
            <div class="content has-text-centered">
              <img src="static/images/results-examples/54.png" alt="" width="40%"/>
            </div>
          </div>
          <div class="box m-5">
            <div class="content has-text-centered">
              <img src="static/images/results-examples/55.png" alt="" width="55%"/>
            </div>
          </div>
          <div class="box m-5">
            <div class="content has-text-centered">
              <img src="static/images/results-examples/56.png" alt="" width="50%"/>
            </div>
          </div>   

        </div>
      </div>
    </div>

    <div class="container is-full has-text-centered content m-6" id="result-table">
      <h2 class="title is-3" id="explorer">Explorer</h2>
      <p>Explore the outputs of each model on <img src="static/images/mathvista.png" style="width:1.0em;vertical-align: middle" alt="Logo"/>
        <span class="mathvista">MathVista</span></p>
      <div class="level has-text-centered" style="position: sticky; top: 0; z-index: 20;">
        <div class="level-item box m-3" style="width: 30%; background: rgba(250, 250, 250, 1);">
          <button class="button" style="width: 100%; border: none; background: rgba(250, 250, 250, 1);" id="refresh-qids">
            <span class="icon is-large">
              <i class="fa fa-redo fa-lg" aria-hidden="true"></i>
            </span>
            <p class="title is-4 m-0">Refresh Question</p>
          </button>
        </div>
        <div class="level-item box m-3" style="width: 30%; background: rgba(250, 250, 250, 1);">
          <div class="dropdown" style="width: 100%;">
            <div class="dropdown-trigger has-text-justified" style="width: 100%; ">
              <button class="button" aria-haspopup="true" aria-controls="dropdown-menu" style="width: 100%; border: none; background: rgba(250, 250, 250, 1);">
                <p class="title m-0 is-4 dropdown-display">Multimodal Bard</p>
                <span class="icon is-large" style="position: absolute; right:0;">
                  <i class="fas fa-angle-down fa-lg" aria-hidden="true"></i>
                </span>
              </button>
            </div>
            <div class="dropdown-menu" id="dropdown-menu" role="menu" style="width:100%;">
              <div class="dropdown-content">
                <!-- <a class="dropdown-item">
                  Dropdown item
                </a>
                <a class="dropdown-item">
                  Other dropdown item
                </a> -->
              </div>
            </div>
          </div>
        </div>
        <div class="level-item box m-3" style="width: 30%; background: rgba(250, 250, 250, 1);">
          <div class="dropdown" style="width: 100%;">
            <div class="dropdown-trigger has-text-justified" style="width: 100%;">
              <button class="button" aria-haspopup="true" aria-controls="dropdown-menu" style="width: 100%; border: none; background: rgba(250, 250, 250, 1);">
                <p class="title m-0 is-4 dropdown-display">CoT GPT4 (Caption+OCR)</p>
                <span class="icon is-large" style="position: absolute; right:0;">
                  <i class="fas fa-angle-down fa-lg" aria-hidden="true"></i>
                </span>
              </button>
            </div>
            <div class="dropdown-menu" id="dropdown-menu" role="menu" style="width:100%;">
              <div class="dropdown-content">
                <!-- <a class="dropdown-item">
                  Dropdown item
                </a>
                <a class="dropdown-item">
                  Other dropdown item
                </a> -->
              </div>
            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>


<!-- @PAN TODO: bibtex -->
<section class="section" id="BibTeX">
  <div class="container is-max-desktop content">
    <h2 class="title is-3 has-text-centered">BibTeX</h2>
    <pre><code>@inproceedings{lu2024mathvista,
  author    = {Lu, Pan and Bansal, Hritik and Xia, Tony and Liu, Jiacheng and Li, Chunyuan and Hajishirzi, Hannaneh and Cheng, Hao and Chang, Kai-Wei and Galley, Michel and Gao, Jianfeng},
  title     = {MathVista: Evaluating Mathematical Reasoning of Foundation Models in Visual Contexts},
  booktitle={International Conference on Learning Representations (ICLR)},
  year      = {2024}
}</code></pre>
  </div>
</section>

<section>
  <div class="section" id="org-banners" style="display:flex">
    <a href="https://www.ucla.edu/" target="_blank" rel="external">
        <img class="center-block org-banner" src="static/images/ucla.png">
    </a>
    <a href="https://www.washington.edu/" target="blank" class="ext-link">
        <img class="center-block org-banner" src="static/images/uw.png">
    </a>
    <a href="https://www.microsoft.com/en-us/research/" target="_blank" rel="external">
        <img class="center-block org-banner" src="static/images/microsoft.png">
    </a>
  </div>
</section>


<footer class="footer">
  <!-- <div class="container"> -->
    <div class="content has-text-centered">
    </div>
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">
          <p>
            This website is website adapted from <a href="https://nerfies.github.io/">Nerfies</a>, licensed under a <a rel="license"
                                                href="http://creativecommons.org/licenses/by-sa/4.0/">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>
        </div>
      </div>
    </div>
  <!-- </div> -->
</footer>

</body>
</html>
